{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0efa9e-730b-4d0a-9927-2c187a9178c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, Bidirectional, Concatenate, Dot, Activation, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb789b2-e54d-460f-affe-9d6a9c090d28",
   "metadata": {},
   "source": [
    "<h1>Dataset</h1>\n",
    "<p><a href=\"https://www.kaggle.com/datasets/dariocioni/c4200m/data\"><b>C4 2OOM </b></a> - untuk Grammar Error Correction</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f497035-67c4-4cc6-9a04-0b00e8703e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = [\"input\", \"target\"]\n",
    "\n",
    "n_rows =  100\n",
    "df1 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00000-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df1.columns = new_column_names\n",
    "df2 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00001-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df2.columns = new_column_names\n",
    "df3 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00002-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df3.columns = new_column_names\n",
    "df4 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00003-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df4.columns = new_column_names\n",
    "df5 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00004-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df5.columns = new_column_names\n",
    "df6 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00005-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df6.columns = new_column_names\n",
    "df7 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00006-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df7.columns = new_column_names\n",
    "df8 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00007-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df8.columns = new_column_names\n",
    "df9 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00008-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df9.columns = new_column_names\n",
    "df10 = pd.read_csv('C:/Users/Azzriel/Desktop/Language Chatbot/data/C4 200M/C4_200M.tsv-00009-of-00010',sep = '\\t',nrows = n_rows)\n",
    "df10.columns = new_column_names\n",
    "data = pd.concat((df1,df2,df3,df4,df5,df6,df7,df8,df9,df10),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c10320-9c9d-4b6e-9c4c-faffb8a3a0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The effect of widespread dud targets two face ...</td>\n",
       "      <td>1. The effect of \"widespread dud\" targets two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tax on sales of stores for non residents are s...</td>\n",
       "      <td>Capital Gains tax on the sale of properties fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Much many brands and sellers still in the market.</td>\n",
       "      <td>Many brands and sellers still in the market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is is the latest Maintenance release of S...</td>\n",
       "      <td>This is is the latest maintenance release of S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fairy Or Not, I'm the Godmother: no just look,...</td>\n",
       "      <td>Fairy Or Not, I'm the Godmother: Not just a lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  The effect of widespread dud targets two face ...   \n",
       "1  tax on sales of stores for non residents are s...   \n",
       "2  Much many brands and sellers still in the market.   \n",
       "3  this is is the latest Maintenance release of S...   \n",
       "4  Fairy Or Not, I'm the Godmother: no just look,...   \n",
       "\n",
       "                                              target  \n",
       "0  1. The effect of \"widespread dud\" targets two ...  \n",
       "1  Capital Gains tax on the sale of properties fo...  \n",
       "2       Many brands and sellers still in the market.  \n",
       "3  This is is the latest maintenance release of S...  \n",
       "4  Fairy Or Not, I'm the Godmother: Not just a lo...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9ac2ca-2a9e-4fdd-bea9-46a3bc700b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf59a93-2f67-41fd-8c19-9e8616b20609",
   "metadata": {},
   "source": [
    "<h2>Data Cleaning</h2>\n",
    "<h3>Standardisasi data</h3>\n",
    "<ul>\n",
    "<li>huruf kata lowercase</li>\n",
    "<li>simbol koma diubah menjadi COMMA</li>\n",
    "<li>punktuasi dihilangkan</li>\n",
    "<li>nomor dihilangkan</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f370589-c9d6-46a7-b414-29551762f112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(lines):\n",
    "  lines.input=lines.input.apply(lambda x: x.lower())\n",
    "  lines.target=lines.target.apply(lambda x: x.lower())\n",
    "\n",
    "  lines.input=lines.input.apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "  lines.target=lines.target.apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "\n",
    "  exclude = str.maketrans('', '',string.punctuation)\n",
    "  lines.input=lines.input.apply(lambda x: x.translate(exclude))\n",
    "  lines.target=lines.target.apply(lambda x: x.translate(exclude))\n",
    "\n",
    "  remove_digits = str.maketrans('', '', digits)\n",
    "  lines.input=lines.input.apply(lambda x: x.translate(remove_digits))\n",
    "  lines.target=lines.target.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94842aa6-3e44-4d9c-8330-a3175b6b4352",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14088b6-56d3-4442-a050-89a6958b5994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the effect of widespread dud targets two face ...</td>\n",
       "      <td>the effect of widespread dud targets two face...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tax on sales of stores for non residents are s...</td>\n",
       "      <td>capital gains tax on the sale of properties fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>much many brands and sellers still in the market</td>\n",
       "      <td>many brands and sellers still in the market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is is the latest maintenance release of s...</td>\n",
       "      <td>this is is the latest maintenance release of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fairy or not COMMA im the godmother no just lo...</td>\n",
       "      <td>fairy or not COMMA im the godmother not just a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  the effect of widespread dud targets two face ...   \n",
       "1  tax on sales of stores for non residents are s...   \n",
       "2   much many brands and sellers still in the market   \n",
       "3  this is is the latest maintenance release of s...   \n",
       "4  fairy or not COMMA im the godmother no just lo...   \n",
       "\n",
       "                                              target  \n",
       "0   the effect of widespread dud targets two face...  \n",
       "1  capital gains tax on the sale of properties fo...  \n",
       "2        many brands and sellers still in the market  \n",
       "3  this is is the latest maintenance release of s...  \n",
       "4  fairy or not COMMA im the godmother not just a...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57592fb0-ac53-464e-b372-7933fb829475",
   "metadata": {},
   "source": [
    "<h2>Tokenization</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b897d465-ba95-4587-aeaf-2997b9362fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                input  \\\n",
      "0   the effect of widespread dud targets two face ...   \n",
      "1   tax on sales of stores for non residents are s...   \n",
      "2    much many brands and sellers still in the market   \n",
      "3   this is is the latest maintenance release of s...   \n",
      "4   fairy or not COMMA im the godmother no just lo...   \n",
      "..                                                ...   \n",
      "95  removing everything from shimmer to longwear e...   \n",
      "96  while yankees are high on study prospect gleyb...   \n",
      "97  our buddies chefs take on a classic rign rob f...   \n",
      "98  and COMMA the gulf between the “digital haves”...   \n",
      "99   oh COMMA you know just hangting out in hogsmeade   \n",
      "\n",
      "                                               target  \\\n",
      "0    the effect of widespread dud targets two face...   \n",
      "1   capital gains tax on the sale of properties fo...   \n",
      "2         many brands and sellers still in the market   \n",
      "3   this is is the latest maintenance release of s...   \n",
      "4   fairy or not COMMA im the godmother not just a...   \n",
      "..                                                ...   \n",
      "95  removing everything from shimmer to longwear e...   \n",
      "96  while the yankees are high on stud prospect gl...   \n",
      "97  our budding chefs take on a classic rignrob fa...   \n",
      "98  and COMMA the gulf between the “digital haves”...   \n",
      "99    oh COMMA you know just hanging out in hogsmeade   \n",
      "\n",
      "                                         input_tokens  \\\n",
      "0   (tf.Tensor(b'the', shape=(), dtype=string), tf...   \n",
      "1   (tf.Tensor(b'tax', shape=(), dtype=string), tf...   \n",
      "2   (tf.Tensor(b'much', shape=(), dtype=string), t...   \n",
      "3   (tf.Tensor(b'this', shape=(), dtype=string), t...   \n",
      "4   (tf.Tensor(b'fairy', shape=(), dtype=string), ...   \n",
      "..                                                ...   \n",
      "95  (tf.Tensor(b'removing', shape=(), dtype=string...   \n",
      "96  (tf.Tensor(b'while', shape=(), dtype=string), ...   \n",
      "97  (tf.Tensor(b'our', shape=(), dtype=string), tf...   \n",
      "98  (tf.Tensor(b'and', shape=(), dtype=string), tf...   \n",
      "99  (tf.Tensor(b'oh', shape=(), dtype=string), tf....   \n",
      "\n",
      "                                        target_tokens  \n",
      "0   (tf.Tensor(b'the', shape=(), dtype=string), tf...  \n",
      "1   (tf.Tensor(b'capital', shape=(), dtype=string)...  \n",
      "2   (tf.Tensor(b'many', shape=(), dtype=string), t...  \n",
      "3   (tf.Tensor(b'this', shape=(), dtype=string), t...  \n",
      "4   (tf.Tensor(b'fairy', shape=(), dtype=string), ...  \n",
      "..                                                ...  \n",
      "95  (tf.Tensor(b'removing', shape=(), dtype=string...  \n",
      "96  (tf.Tensor(b'while', shape=(), dtype=string), ...  \n",
      "97  (tf.Tensor(b'our', shape=(), dtype=string), tf...  \n",
      "98  (tf.Tensor(b'and', shape=(), dtype=string), tf...  \n",
      "99  (tf.Tensor(b'oh', shape=(), dtype=string), tf....  \n",
      "\n",
      "[1000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    tokens = tf.strings.split(sentence)\n",
    "    return tokens\n",
    "\n",
    "data['input_tokens'] = data['input'].apply(tokenize_sentence)\n",
    "\n",
    "data['target_tokens'] = data['target'].apply(tokenize_sentence)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523d1aa-276e-4073-92a2-9b65708f11c9",
   "metadata": {},
   "source": [
    "<h1>Numerization</h1\n",
    "<h2>GLoVe embedding <a href=\"https://nlp.stanford.edu/projects/glove/\"> download </a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8daec11c-e05a-48d0-b65b-5d52205aae8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                input  \\\n",
      "0   the effect of widespread dud targets two face ...   \n",
      "1   tax on sales of stores for non residents are s...   \n",
      "2    much many brands and sellers still in the market   \n",
      "3   this is is the latest maintenance release of s...   \n",
      "4   fairy or not COMMA im the godmother no just lo...   \n",
      "..                                                ...   \n",
      "95  removing everything from shimmer to longwear e...   \n",
      "96  while yankees are high on study prospect gleyb...   \n",
      "97  our buddies chefs take on a classic rign rob f...   \n",
      "98  and COMMA the gulf between the “digital haves”...   \n",
      "99   oh COMMA you know just hangting out in hogsmeade   \n",
      "\n",
      "                                               target  \\\n",
      "0    the effect of widespread dud targets two face...   \n",
      "1   capital gains tax on the sale of properties fo...   \n",
      "2         many brands and sellers still in the market   \n",
      "3   this is is the latest maintenance release of s...   \n",
      "4   fairy or not COMMA im the godmother not just a...   \n",
      "..                                                ...   \n",
      "95  removing everything from shimmer to longwear e...   \n",
      "96  while the yankees are high on stud prospect gl...   \n",
      "97  our budding chefs take on a classic rignrob fa...   \n",
      "98  and COMMA the gulf between the “digital haves”...   \n",
      "99    oh COMMA you know just hanging out in hogsmeade   \n",
      "\n",
      "                                         input_tokens  \\\n",
      "0   (tf.Tensor(b'the', shape=(), dtype=string), tf...   \n",
      "1   (tf.Tensor(b'tax', shape=(), dtype=string), tf...   \n",
      "2   (tf.Tensor(b'much', shape=(), dtype=string), t...   \n",
      "3   (tf.Tensor(b'this', shape=(), dtype=string), t...   \n",
      "4   (tf.Tensor(b'fairy', shape=(), dtype=string), ...   \n",
      "..                                                ...   \n",
      "95  (tf.Tensor(b'removing', shape=(), dtype=string...   \n",
      "96  (tf.Tensor(b'while', shape=(), dtype=string), ...   \n",
      "97  (tf.Tensor(b'our', shape=(), dtype=string), tf...   \n",
      "98  (tf.Tensor(b'and', shape=(), dtype=string), tf...   \n",
      "99  (tf.Tensor(b'oh', shape=(), dtype=string), tf....   \n",
      "\n",
      "                                        target_tokens  \\\n",
      "0   (tf.Tensor(b'the', shape=(), dtype=string), tf...   \n",
      "1   (tf.Tensor(b'capital', shape=(), dtype=string)...   \n",
      "2   (tf.Tensor(b'many', shape=(), dtype=string), t...   \n",
      "3   (tf.Tensor(b'this', shape=(), dtype=string), t...   \n",
      "4   (tf.Tensor(b'fairy', shape=(), dtype=string), ...   \n",
      "..                                                ...   \n",
      "95  (tf.Tensor(b'removing', shape=(), dtype=string...   \n",
      "96  (tf.Tensor(b'while', shape=(), dtype=string), ...   \n",
      "97  (tf.Tensor(b'our', shape=(), dtype=string), tf...   \n",
      "98  (tf.Tensor(b'and', shape=(), dtype=string), tf...   \n",
      "99  (tf.Tensor(b'oh', shape=(), dtype=string), tf....   \n",
      "\n",
      "                                     input_embeddings  \\\n",
      "0   [[0.04656, 0.21318, -0.0074364, -0.45854, -0.0...   \n",
      "1   [[-0.38176, 0.11216, -0.12193, 0.70097, 0.4875...   \n",
      "2   [[-0.20118, -0.13656, -0.069752, -0.067993, 0....   \n",
      "3   [[-0.20437, 0.16431, 0.041794, -0.13708, -0.29...   \n",
      "4   [[-0.85088, -0.46658, 0.099967, -0.37111, 0.09...   \n",
      "..                                                ...   \n",
      "95  [[0.096196, -0.31547, 0.091553, -0.31081, -0.2...   \n",
      "96  [[-0.19472, 0.18836, 0.11739, -0.0018991, -0.1...   \n",
      "97  [[-0.025179, 0.07837, -0.17637, -0.12285, -0.1...   \n",
      "98  [[0.038466, -0.039792, 0.082747, -0.38923, -0....   \n",
      "99  [[-0.14096, -0.43749, 0.035109, 0.049868, -0.8...   \n",
      "\n",
      "                                    target_embeddings  \n",
      "0   [[0.04656, 0.21318, -0.0074364, -0.45854, -0.0...  \n",
      "1   [[0.55308, -0.31169, -0.54397, -0.42538, 0.390...  \n",
      "2   [[-0.15507, 0.28422, 0.31191, -0.19596, 0.1257...  \n",
      "3   [[-0.20437, 0.16431, 0.041794, -0.13708, -0.29...  \n",
      "4   [[-0.85088, -0.46658, 0.099967, -0.37111, 0.09...  \n",
      "..                                                ...  \n",
      "95  [[0.096196, -0.31547, 0.091553, -0.31081, -0.2...  \n",
      "96  [[-0.19472, 0.18836, 0.11739, -0.0018991, -0.1...  \n",
      "97  [[-0.025179, 0.07837, -0.17637, -0.12285, -0.1...  \n",
      "98  [[0.038466, -0.039792, 0.082747, -0.38923, -0....  \n",
      "99  [[-0.14096, -0.43749, 0.035109, 0.049868, -0.8...  \n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('C:/Users/Azzriel/Desktop/Language Chatbot/v4/glove.6B.300d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "def tokens_to_embeddings(tokens):\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        embedding = embeddings_index.get(token.numpy().decode('utf-8'))\n",
    "        if embedding is not None:\n",
    "            embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "data['input_embeddings'] = data['input_tokens'].apply(tokens_to_embeddings)\n",
    "data['target_embeddings'] = data['target_tokens'].apply(tokens_to_embeddings)\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2acff17-c5ae-4431-ba00-b1c2a4a114af",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 100  \n",
    "input_embeddings_padded = pad_sequences(data['input_embeddings'], maxlen=max_sequence_length)\n",
    "target_embeddings_padded = pad_sequences(data['target_embeddings'], maxlen=max_sequence_length)\n",
    "\n",
    "x = np.expand_dims(input_embeddings_padded, axis=-1)\n",
    "y = np.expand_dims(target_embeddings_padded, axis=-1)\n",
    "\n",
    "padded_data = pd.DataFrame({\n",
    "    'input_embeddings_padded': list(x),\n",
    "    'target_embeddings_padded': list(y)\n",
    "})\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ced18c-68d1-4513-869b-8ea55c8c2e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">219,648</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]        │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, │         <span style=\"color: #00af00; text-decoration-color: #00af00\">219,648</span> │ input_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]        │                 │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>], lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                   │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │         \u001b[38;5;34m219,648\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│                               │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]        │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                 │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, │         \u001b[38;5;34m219,648\u001b[0m │ input_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       │\n",
       "│                               │ \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]        │                 │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m], lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)            │             \u001b[38;5;34m129\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">439,425</span> (1.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m439,425\u001b[0m (1.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">439,425</span> (1.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m439,425\u001b[0m (1.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_dim = 300  \n",
    "latent_dim = 128\n",
    "\n",
    "encoder_inputs = Input(shape=(max_sequence_length, input_dim))\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_sequence_length, input_dim))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(y.shape[-1])  # Output layer\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')  \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55304493-22f6-4fca-8bf9-a4ffbfc629a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 651ms/step - loss: 0.0050 - val_loss: 0.0036\n",
      "Epoch 2/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 724ms/step - loss: 0.0040 - val_loss: 0.0036\n",
      "Epoch 3/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 910ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 4/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 872ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 5/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 730ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 6/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 903ms/step - loss: 0.0039 - val_loss: 0.0035\n",
      "Epoch 7/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 830ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 8/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 687ms/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 9/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 724ms/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 10/10\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 655ms/step - loss: 0.0038 - val_loss: 0.0035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1d0b5b07770>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x_train, y_train[:, :100]], y_train[:, :100], \n",
    "          validation_data=([x_val, y_val[:, :100]], y_val[:, :100]), \n",
    "          batch_size=64, \n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10a36eeb-e55a-4583-b57c-6e3ac276afd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corrected_output_string\n\u001b[0;32m     22\u001b[0m input_string \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis are example sentences.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an example sentence.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 23\u001b[0m corrected_string \u001b[38;5;241m=\u001b[39m \u001b[43mcorrect_grammar\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(corrected_string)\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36mcorrect_grammar\u001b[1;34m(input_string, model)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcorrect_grammar\u001b[39m(input_string, model):\n\u001b[1;32m---> 12\u001b[0m     preprocessed_input \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_input_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     corrected_output_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(preprocessed_input)\n\u001b[0;32m     16\u001b[0m     corrected_output_tokens \u001b[38;5;241m=\u001b[39m embeddings_to_tokens(corrected_output_embedding)\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mpreprocess_input_string\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input_string\u001b[39m(input_string):\n\u001b[0;32m      2\u001b[0m     input_tokens \u001b[38;5;241m=\u001b[39m tokenize_sentence(input_string)\n\u001b[1;32m----> 4\u001b[0m     input_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtokens_to_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \n\u001b[0;32m      7\u001b[0m     padded_input_embeddings \u001b[38;5;241m=\u001b[39m pad_sequences([input_embeddings], maxlen\u001b[38;5;241m=\u001b[39mmax_sequence_length)\n",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mtokens_to_embeddings\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     10\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m---> 12\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m embeddings_index\u001b[38;5;241m.\u001b[39mget(\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m embedding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mappend(embedding)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "def preprocess_input_string(input_string):\n",
    "    input_tokens = tokenize_sentence(input_string)\n",
    "    \n",
    "    input_embeddings = tokens_to_embeddings(input_tokens)\n",
    "    \n",
    "    max_sequence_length = 100  \n",
    "    padded_input_embeddings = pad_sequences([input_embeddings], maxlen=max_sequence_length)\n",
    "    \n",
    "    return padded_input_embeddings\n",
    "\n",
    "def correct_grammar(input_string, model):\n",
    "    preprocessed_input = preprocess_input_string(input_string)\n",
    "    \n",
    "    corrected_output_embedding = model.predict(preprocessed_input)\n",
    "    \n",
    "    corrected_output_tokens = embeddings_to_tokens(corrected_output_embedding)\n",
    "    \n",
    "    corrected_output_string = ' '.join(corrected_output_tokens)\n",
    "    \n",
    "    return corrected_output_string\n",
    "\n",
    "input_string = [\"This are example sentences.\", \"This is an example sentence.\"]\n",
    "corrected_string = correct_grammar(input_string, model)\n",
    "print(corrected_string)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
